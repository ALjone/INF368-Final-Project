{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:\n",
      "avg_train_loss 23.042459321022033\n",
      "elapsed time for 1 training epoch :  0:01:21\n",
      "avg_train_loss 3.345381832122803\n",
      "elapsed time for 1 training epoch :  0:01:13\n",
      "avg_train_loss 2.6946317851543427\n",
      "elapsed time for 1 training epoch :  0:01:13\n",
      "avg_train_loss 2.4691387176513673\n",
      "elapsed time for 1 training epoch :  0:01:20\n",
      "avg_train_loss 2.3011879861354827\n",
      "elapsed time for 1 training epoch :  0:01:15\n",
      "avg_train_loss 2.1269299149513246\n",
      "elapsed time for 1 training epoch :  0:01:15\n",
      "avg_train_loss 1.9707930505275726\n",
      "elapsed time for 1 training epoch :  0:01:16\n",
      "avg_train_loss 1.8922729790210724\n",
      "elapsed time for 1 training epoch :  0:01:14\n",
      "avg_train_loss 1.7725751638412475\n",
      "elapsed time for 1 training epoch :  0:01:23\n",
      "avg_train_loss 1.6596145272254943\n",
      "elapsed time for 1 training epoch :  0:01:24\n",
      "Generating sentences\n",
      "Finished generating sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path ./data/train_labeled_5.csv \\\n",
    "--output_name generated_samples_5.txt \\\n",
    "--epochs 10 \\\n",
    "--batch_size 1 \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg_train_loss 10.755517369508743\n",
      "elapsed time for 1 training epoch :  0:01:43\n",
      "avg_train_loss 2.440124177932739\n",
      "elapsed time for 1 training epoch :  0:01:35\n",
      "avg_train_loss 1.9912845328450204\n",
      "elapsed time for 1 training epoch :  0:01:35\n",
      "avg_train_loss 1.8234325051307678\n",
      "elapsed time for 1 training epoch :  0:01:36\n",
      "avg_train_loss 1.6345902353525161\n",
      "elapsed time for 1 training epoch :  0:01:35\n",
      "Generating sentences\n",
      "Finished generating sentences.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path ./data/train_labeled_10.csv \\\n",
    "--output_name generated_samples_10.txt \\\n",
    "--epochs 10 \\\n",
    "--batch_size 1 \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"GPT2Tuner.py\", line 142, in <module>\n",
      "    tuner.save_sentences(int(args.samples_per_class), path=args.output_dir + \"/\" + args.output_name)\n",
      "  File \"GPT2Tuner.py\", line 113, in save_sentences\n",
      "    f.write(self.tokenizer.decode(sample_output, skip_special_tokens=True).replace(\"\\n\", \"\")+\"\\n\")\n",
      "  File \"D:\\Anaconda\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\x96' in position 396: character maps to <undefined>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg_train_loss 5.252679090499878\n",
      "elapsed time for 1 training epoch :  0:10:09\n",
      "avg_train_loss 1.7242548483610154\n",
      "elapsed time for 1 training epoch :  0:10:03\n",
      "avg_train_loss 1.6241241866350173\n",
      "elapsed time for 1 training epoch :  0:09:44\n",
      "avg_train_loss 1.470165878534317\n",
      "elapsed time for 1 training epoch :  0:10:09\n",
      "avg_train_loss 1.2941617393493652\n",
      "elapsed time for 1 training epoch :  0:09:03\n",
      "avg_train_loss 1.1303855109214782\n",
      "elapsed time for 1 training epoch :  0:09:02\n",
      "avg_train_loss 0.9625671485066414\n",
      "elapsed time for 1 training epoch :  0:09:03\n",
      "avg_train_loss 0.8146653261780739\n",
      "elapsed time for 1 training epoch :  0:09:11\n",
      "avg_train_loss 0.6817608812451362\n",
      "elapsed time for 1 training epoch :  0:11:03\n",
      "avg_train_loss 0.5570713968575001\n",
      "elapsed time for 1 training epoch :  0:16:43\n",
      "Generating sentences\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path ./data/train_labeled_25.csv \\\n",
    "--output_name generated_samples_25.txt \\\n",
    "--epochs 10 \\\n",
    "--batch_size 1 \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:\n",
      "avg_train_loss 3.7483344689011573\n",
      "elapsed time for 1 training epoch :  0:12:29\n",
      "avg_train_loss 1.6732652413845062\n",
      "elapsed time for 1 training epoch :  0:14:36\n",
      "avg_train_loss 1.2771680292487144\n",
      "elapsed time for 1 training epoch :  0:19:47\n",
      "avg_train_loss 1.023910652846098\n",
      "elapsed time for 1 training epoch :  0:27:10\n",
      "avg_train_loss 0.8440950420498848\n",
      "elapsed time for 1 training epoch :  0:49:47\n",
      "Generating sentences\n",
      "Finished generating sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path ./data/train_labeled_50.csv \\\n",
    "--output_name generated_samples_50.txt \\\n",
    "--epochs 10 \\\n",
    "--batch_size 1 \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
