{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.3.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -q -U tensorflow-text\n",
    "!pip install -q tf-models-official\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('..')\n",
    "from Bert import Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Starting training:\n",
      "avg_train_loss 35.90432729721069\n",
      "elapsed time for 1 training epoch :  0:00:05\n",
      "avg_train_loss 8.821808385849\n",
      "elapsed time for 1 training epoch :  0:00:05\n",
      "Generating sentences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sentences.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_5.csv \\\n",
    "--output_name generated_samples_5.txt \\\n",
    "--epochs 2 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Starting training:\n",
      "avg_train_loss 21.648864412307738\n",
      "elapsed time for 1 training epoch :  0:00:09\n",
      "avg_train_loss 2.1943873286247255\n",
      "elapsed time for 1 training epoch :  0:00:09\n",
      "Generating sentences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sentences.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_10.csv \\\n",
    "--output_name generated_samples_10.txt \\\n",
    "--epochs 2 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Starting training:\n",
      "avg_train_loss 10.05415738105774\n",
      "elapsed time for 1 training epoch :  0:00:32\n",
      "avg_train_loss 1.586916537284851\n",
      "elapsed time for 1 training epoch :  0:00:32\n",
      "Generating sentences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sentences.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_25.csv \\\n",
    "--output_name generated_samples_25.txt \\\n",
    "--epochs 2 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Starting training:\n",
      "avg_train_loss 4.690613156557083\n",
      "elapsed time for 1 training epoch :  0:01:05\n",
      "avg_train_loss 1.2576002192497253\n",
      "elapsed time for 1 training epoch :  0:01:05\n",
      "Generating sentences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sentences.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_50.csv \\\n",
    "--output_name generated_samples_50.txt \\\n",
    "--epochs 2 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seed = 0\n",
    "learning_rate = 5e-5\n",
    "epochs=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_sentences(train_path, synthesized_path, sentences_per_label):\n",
    "        # todo: num_classes\n",
    "        h = Bert(num_classes = 2, random_state = seed)\n",
    "        train = pd.read_csv(train_path)\n",
    "        h.train(train.text, train.label,learning_rate=learning_rate,batch_size=batch_size,epochs=epochs) \n",
    "\n",
    "        with open(synthesized_path, \"r\") as file:\n",
    "            sentences = file.readlines()\n",
    "        \n",
    "        \n",
    "        labels = []\n",
    "        cleaned_sentences = []\n",
    "        for i,sentence in enumerate(sentences):\n",
    "            #sentence_parts = sentence.split(maxsplit = 1)\n",
    "            if len(sentence[3:]) <10:\n",
    "                continue\n",
    "            labels.append(sentence[:3])\n",
    "            cleaned_sentences.append(sentence[3:])\n",
    "\n",
    "\n",
    "        \n",
    "        predictions = h.predict_label_proba(pd.DataFrame(cleaned_sentences, columns=[\"text\"]))\n",
    "        pred = [pred[0] for pred in predictions]\n",
    "        conf = [pred[1] for pred in predictions]\n",
    "        data = {\"text\":cleaned_sentences,\"label\":labels,\"predicted label\":pred, \"confidence\":conf}\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        candidates = df.loc[df[\"label\"] == df[\"predicted label\"]]\n",
    "        candidates = candidates.sort_values([\"confidence\"],ascending=False).groupby([\"label\"]).head(sentences_per_label)    \n",
    "\n",
    "        return candidates[[\"text\", \"label\", \"confidence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 07:54:48.882367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10800 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7\n",
      "2021-11-11 07:54:52.448310: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 21s 260ms/step - loss: 0.6874 - accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.7369 - accuracy: 0.7000\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.5518 - accuracy: 0.6000\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.2172 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.1872 - accuracy: 1.0000\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "2\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 22s 291ms/step - loss: 0.6853 - accuracy: 0.7000\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 1s 279ms/step - loss: 0.7629 - accuracy: 0.5500\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 1s 277ms/step - loss: 0.4207 - accuracy: 0.8500\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 1s 279ms/step - loss: 0.2407 - accuracy: 0.9500\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 0.1949 - accuracy: 0.9500\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "6\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - 24s 279ms/step - loss: 0.9113 - accuracy: 0.5400\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 4s 273ms/step - loss: 1.0443 - accuracy: 0.4400\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 0.6221 - accuracy: 0.6200\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 0.4343 - accuracy: 0.8400\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 0.2037 - accuracy: 1.0000\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "12\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 27s 281ms/step - loss: 0.8631 - accuracy: 0.5200\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.6664 - accuracy: 0.5900\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 7s 279ms/step - loss: 0.4740 - accuracy: 0.7300\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.2916 - accuracy: 0.8900\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.0203 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for datapoints in [5,10,25,50]:\n",
    "    get_sentences(\"data/train_labeled_\"+str(datapoints) + \".csv\",\n",
    "                \"data/generated_samples_\" + str(datapoints)+\".txt\",\n",
    "                  datapoints).to_csv(\"data/filtered_data_\" + str(datapoints)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie was extremely weak and did not del...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sure this movie wasn't like. 16 blocks, inside...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In this day and age of incredible special movi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Theory Of Flight is an entertaining and e...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is an extremely long movie, which means y...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>My first reaction upon seeing this movie was ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>This is one of my favourite martial arts movie...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>This was so lame that I turned the DVD off...m...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>This movie is extremely disturbing and frankl...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>The three main characters are very well portra...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text label\n",
       "0     This movie was extremely weak and did not del...   pos\n",
       "1    Sure this movie wasn't like. 16 blocks, inside...   pos\n",
       "2    In this day and age of incredible special movi...   neg\n",
       "3     The Theory Of Flight is an entertaining and e...   pos\n",
       "4    This is an extremely long movie, which means y...   pos\n",
       "..                                                 ...   ...\n",
       "195   My first reaction upon seeing this movie was ...   pos\n",
       "196  This is one of my favourite martial arts movie...   pos\n",
       "197  This was so lame that I turned the DVD off...m...   neg\n",
       "198   This movie is extremely disturbing and frankl...   neg\n",
       "199  The three main characters are very well portra...   pos\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoints = 50\n",
    "train = pd.read_csv(\"data/filtered_data_\" + str(datapoints)+\".csv\")[[\"text\",\"label\"]]\n",
    "train = pd.concat([train,pd.read_csv(\"data/train_labeled_\" + str(datapoints)+\".csv\")]) \n",
    "train = train.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "train.label[0] = \"pos\"\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "2\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 22s 290ms/step - loss: 0.8798 - accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 0.4049 - accuracy: 0.9000\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 0.1701 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 1s 279ms/step - loss: 0.1415 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 0.0973 - accuracy: 1.0000\n",
      "500/500 [==============================] - 18s 33ms/step - loss: 0.8370 - accuracy: 0.5500\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "5\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 23s 284ms/step - loss: 0.7531 - accuracy: 0.7250\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 3s 279ms/step - loss: 0.2386 - accuracy: 0.9000\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 3s 279ms/step - loss: 0.2570 - accuracy: 0.9250\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 3s 279ms/step - loss: 0.1297 - accuracy: 0.9500\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 3s 279ms/step - loss: 0.1026 - accuracy: 0.9750\n",
      "500/500 [==============================] - 18s 33ms/step - loss: 1.0745 - accuracy: 0.5980\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "12\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 29s 282ms/step - loss: 0.6764 - accuracy: 0.6600\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.2385 - accuracy: 0.8900\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 7s 279ms/step - loss: 0.0934 - accuracy: 0.9800\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 7s 279ms/step - loss: 0.0229 - accuracy: 0.9900\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 8.3696e-04 - accuracy: 1.0000\n",
      "500/500 [==============================] - 18s 33ms/step - loss: 1.5090 - accuracy: 0.71200s - los\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "25\n",
      "Epoch 1/5\n",
      "50/50 [==============================] - 34s 280ms/step - loss: 0.7314 - accuracy: 0.6350\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 14s 280ms/step - loss: 0.4493 - accuracy: 0.8600\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 14s 281ms/step - loss: 0.1375 - accuracy: 0.9700\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 14s 281ms/step - loss: 0.0180 - accuracy: 0.9950\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 14s 281ms/step - loss: 0.0136 - accuracy: 0.9950\n",
      "500/500 [==============================] - 18s 33ms/step - loss: 1.4785 - accuracy: 0.7680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_per_class</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_per_class  accuracy\n",
       "0          5.0     0.550\n",
       "1         10.0     0.598\n",
       "2         25.0     0.712\n",
       "3         50.0     0.768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(columns=[\"n_per_class\", \"accuracy\"])\n",
    "for datapoints in [50]:\n",
    "    h = Bert(num_classes = 2, random_state = seed)\n",
    "    train = pd.read_csv(\"data/filtered_data_\" + str(datapoints)+\".csv\")[[\"text\",\"label\"]]\n",
    "    train = pd.concat([pd.read_csv(\"data/train_labeled_\" + str(datapoints)+\".csv\"),train]) \n",
    "    train = train.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    train.label[0] = \"pos\"\n",
    "    h.train(train.text, train.label, learning_rate=learning_rate,batch_size=batch_size,epochs=epochs)  \n",
    "    performance = h.evaluate_from_path(\"data/test.csv\")[1]\n",
    "    row = {\"n_per_class\" : datapoints, \"accuracy\": performance}\n",
    "    data = data.append(row, ignore_index=True)\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results'):\n",
    "      os.mkdir('results')\n",
    "data.to_csv(\"results/LAMBADA_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
