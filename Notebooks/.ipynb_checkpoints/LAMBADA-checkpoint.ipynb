{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.3.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -q -U tensorflow-text\n",
    "!pip install -q tf-models-official\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('..')\n",
    "from Bert import Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_5.csv \\\n",
    "--output_name generated_samples_5.txt \\\n",
    "--epochs 10 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg_train_loss 10.755517369508743\n",
      "elapsed time for 1 training epoch :  0:01:43\n",
      "avg_train_loss 2.440124177932739\n",
      "elapsed time for 1 training epoch :  0:01:35\n",
      "avg_train_loss 1.9912845328450204\n",
      "elapsed time for 1 training epoch :  0:01:35\n",
      "avg_train_loss 1.8234325051307678\n",
      "elapsed time for 1 training epoch :  0:01:36\n",
      "avg_train_loss 1.6345902353525161\n",
      "elapsed time for 1 training epoch :  0:01:35\n",
      "Generating sentences\n",
      "Finished generating sentences.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_10.csv \\\n",
    "--output_name generated_samples_10.txt \\\n",
    "--epochs 10 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"GPT2Tuner.py\", line 142, in <module>\n",
      "    tuner.save_sentences(int(args.samples_per_class), path=args.output_dir + \"/\" + args.output_name)\n",
      "  File \"GPT2Tuner.py\", line 113, in save_sentences\n",
      "    f.write(self.tokenizer.decode(sample_output, skip_special_tokens=True).replace(\"\\n\", \"\")+\"\\n\")\n",
      "  File \"D:\\Anaconda\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\x96' in position 396: character maps to <undefined>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg_train_loss 5.252679090499878\n",
      "elapsed time for 1 training epoch :  0:10:09\n",
      "avg_train_loss 1.7242548483610154\n",
      "elapsed time for 1 training epoch :  0:10:03\n",
      "avg_train_loss 1.6241241866350173\n",
      "elapsed time for 1 training epoch :  0:09:44\n",
      "avg_train_loss 1.470165878534317\n",
      "elapsed time for 1 training epoch :  0:10:09\n",
      "avg_train_loss 1.2941617393493652\n",
      "elapsed time for 1 training epoch :  0:09:03\n",
      "avg_train_loss 1.1303855109214782\n",
      "elapsed time for 1 training epoch :  0:09:02\n",
      "avg_train_loss 0.9625671485066414\n",
      "elapsed time for 1 training epoch :  0:09:03\n",
      "avg_train_loss 0.8146653261780739\n",
      "elapsed time for 1 training epoch :  0:09:11\n",
      "avg_train_loss 0.6817608812451362\n",
      "elapsed time for 1 training epoch :  0:11:03\n",
      "avg_train_loss 0.5570713968575001\n",
      "elapsed time for 1 training epoch :  0:16:43\n",
      "Generating sentences\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_25.csv \\\n",
    "--output_name generated_samples_25.txt \\\n",
    "--epochs 10 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:\n",
      "avg_train_loss 3.7483344689011573\n",
      "elapsed time for 1 training epoch :  0:12:29\n",
      "avg_train_loss 1.6732652413845062\n",
      "elapsed time for 1 training epoch :  0:14:36\n",
      "avg_train_loss 1.2771680292487144\n",
      "elapsed time for 1 training epoch :  0:19:47\n",
      "avg_train_loss 1.023910652846098\n",
      "elapsed time for 1 training epoch :  0:27:10\n",
      "avg_train_loss 0.8440950420498848\n",
      "elapsed time for 1 training epoch :  0:49:47\n",
      "Generating sentences\n",
      "Finished generating sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_50.csv \\\n",
    "--output_name generated_samples_50.txt \\\n",
    "--epochs 10 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seed = 0\n",
    "learning_rate = 5e-5\n",
    "epochs=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_sentences(train_path, synthesized_path, sentences_per_label):\n",
    "        # todo: num_classes\n",
    "        h = Bert(num_classes = 2, random_state = seed)\n",
    "        train = pd.read_csv(train_path)\n",
    "        h.train(train.text, train.label,learning_rate=learning_rate,batch_size=batch_size,epochs=epochs) \n",
    "\n",
    "        with open(synthesized_path, \"r\") as file:\n",
    "            sentences = file.readlines()\n",
    "        \n",
    "        \n",
    "        labels = []\n",
    "        cleaned_sentences = []\n",
    "        for i,sentence in enumerate(sentences):\n",
    "            #sentence_parts = sentence.split(maxsplit = 1)\n",
    "            if len(sentence[3:]) <10:\n",
    "                continue\n",
    "            labels.append(sentence[:3])\n",
    "            cleaned_sentences.append(sentence[3:])\n",
    "\n",
    "\n",
    "        \n",
    "        predictions = h.predict_label_proba(pd.DataFrame(cleaned_sentences, columns=[\"text\"]))\n",
    "        pred = [pred[0] for pred in predictions]\n",
    "        conf = [pred[1] for pred in predictions]\n",
    "        data = {\"text\":cleaned_sentences,\"label\":labels,\"predicted label\":pred, \"confidence\":conf}\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        candidates = df.loc[df[\"label\"] == df[\"predicted label\"]]\n",
    "        candidates = candidates.sort_values([\"confidence\"],ascending=False).groupby([\"label\"]).head(sentences_per_label)    \n",
    "\n",
    "        return candidates[[\"text\", \"label\", \"confidence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 28s 256ms/step - loss: 0.7305 - accuracy: 0.6000\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.6992 - accuracy: 0.6000\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.7635 - accuracy: 0.7000\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.2657 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.1894 - accuracy: 1.0000\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 29s 293ms/step - loss: 0.5837 - accuracy: 0.7000\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 1s 282ms/step - loss: 1.0768 - accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 1s 280ms/step - loss: 0.9412 - accuracy: 0.4500\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 1s 279ms/step - loss: 0.8104 - accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 1s 280ms/step - loss: 0.9005 - accuracy: 0.4500\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - 28s 277ms/step - loss: 0.8901 - accuracy: 0.5600\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 4s 273ms/step - loss: 1.0387 - accuracy: 0.4800\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 0.8938 - accuracy: 0.3800\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 4s 273ms/step - loss: 0.7242 - accuracy: 0.5600\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 4s 273ms/step - loss: 0.6732 - accuracy: 0.6000\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 29s 283ms/step - loss: 0.8718 - accuracy: 0.5100\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 7s 281ms/step - loss: 0.6832 - accuracy: 0.5900\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 7s 281ms/step - loss: 0.4283 - accuracy: 0.7900\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.1537 - accuracy: 0.9300\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.0666 - accuracy: 0.9900\n"
     ]
    }
   ],
   "source": [
    "for datapoints in [5,10,25,50]:\n",
    "    get_sentences(\"data/train_labeled_\"+str(datapoints) + \".csv\",\n",
    "                \"data/generated_samples_\" + str(datapoints)+\".txt\", 100).to_csv(\"data/filtered_data_\" + str(datapoints)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It occurs to me that the depiction of the mai...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an extremely weak martial arts film, ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is extremely weak. If I wanted to ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie was so weak and did not really hve...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The original was extremely weak, and was larg...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Someone(or, something thing..)is leaving punct...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Very good martial arts film and Jet Li is the ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>It's hard to say anything about a movie like t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Red Skelton was still another major star who m...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Theory Of Flight is an engaging character ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text label\n",
       "0    It occurs to me that the depiction of the mai...   neg\n",
       "1    This is an extremely weak martial arts film, ...   neg\n",
       "2    This movie is extremely weak. If I wanted to ...   neg\n",
       "3    This movie was so weak and did not really hve...   neg\n",
       "4    The original was extremely weak, and was larg...   neg\n",
       "..                                                ...   ...\n",
       "95  Someone(or, something thing..)is leaving punct...   pos\n",
       "96  Very good martial arts film and Jet Li is the ...   pos\n",
       "97  It's hard to say anything about a movie like t...   pos\n",
       "98  Red Skelton was still another major star who m...   pos\n",
       "99  The Theory Of Flight is an engaging character ...   pos\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/filtered_data_\" + str(datapoints)+\".csv\")\n",
    "train = pd.concat([train[[\"text\",\"label\"]],pd.read_csv(\"data/train_labeled_\" + str(datapoints)+\".csv\")])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/10\n",
      "75/75 [==============================] - 53s 280ms/step - loss: 0.4977 - accuracy: 0.7500\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 21s 281ms/step - loss: 0.2273 - accuracy: 0.9533\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 21s 282ms/step - loss: 0.1146 - accuracy: 0.9733\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 21s 282ms/step - loss: 0.0286 - accuracy: 0.9967\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 21s 282ms/step - loss: 1.9706e-04 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 21s 283ms/step - loss: 1.6071e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 21s 282ms/step - loss: 2.8900e-05 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 21s 282ms/step - loss: 2.0298e-05 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 21s 283ms/step - loss: 1.4412e-05 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 21s 283ms/step - loss: 1.5783e-05 - accuracy: 1.0000\n",
      "500/500 [==============================] - 19s 32ms/step - loss: 2.2709 - accuracy: 0.7560\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_per_class</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_per_class  accuracy\n",
       "0         50.0     0.756"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "data = pd.DataFrame(columns=[\"n_per_class\", \"accuracy\"])\n",
    "for datapoints in [50]:\n",
    "    h = Bert(num_classes = 2, random_state = seed)\n",
    "    train = pd.read_csv(\"data/filtered_data_\" + str(datapoints)+\".csv\")\n",
    "    train = pd.concat([train[[\"text\",\"label\"]],pd.read_csv(\"data/train_labeled_\" + str(datapoints)+\".csv\")])\n",
    "    h.train(train.text, train.label,learning_rate=learning_rate,batch_size=batch_size,epochs=epochs)  \n",
    "    performance = h.evaluate_from_path(\"data/test.csv\")[1]\n",
    "    row = {\"n_per_class\" : datapoints, \"accuracy\": performance}\n",
    "    data = data.append(row, ignore_index=True)\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results'):\n",
    "      os.mkdir('results')\n",
    "data.to_csv(\"results/LAMBADA_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
