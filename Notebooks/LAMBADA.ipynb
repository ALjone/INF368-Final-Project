{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.3.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -q -U tensorflow-text\n",
    "!pip install -q tf-models-official\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('..')\n",
    "from Bert import Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Starting training:\n",
      "avg_train_loss 35.90432729721069\n",
      "elapsed time for 1 training epoch :  0:00:05\n",
      "avg_train_loss 8.821808385849\n",
      "elapsed time for 1 training epoch :  0:00:05\n",
      "Generating sentences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sentences.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_5.csv \\\n",
    "--output_name generated_samples_5.txt \\\n",
    "--epochs 2 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Starting training:\n",
      "avg_train_loss 21.648864412307738\n",
      "elapsed time for 1 training epoch :  0:00:09\n",
      "avg_train_loss 2.1943873286247255\n",
      "elapsed time for 1 training epoch :  0:00:09\n",
      "Generating sentences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sentences.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_10.csv \\\n",
    "--output_name generated_samples_10.txt \\\n",
    "--epochs 2 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Starting training:\n",
      "avg_train_loss 10.05415738105774\n",
      "elapsed time for 1 training epoch :  0:00:32\n",
      "avg_train_loss 1.586916537284851\n",
      "elapsed time for 1 training epoch :  0:00:32\n",
      "Generating sentences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sentences.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_25.csv \\\n",
    "--output_name generated_samples_25.txt \\\n",
    "--epochs 2 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Starting training:\n",
      "avg_train_loss 4.690613156557083\n",
      "elapsed time for 1 training epoch :  0:01:05\n",
      "avg_train_loss 1.2576002192497253\n",
      "elapsed time for 1 training epoch :  0:01:05\n",
      "Generating sentences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sentences.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_50.csv \\\n",
    "--output_name generated_samples_50.txt \\\n",
    "--epochs 2 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seed = 0\n",
    "learning_rate = 5e-5\n",
    "epochs=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_sentences(train_path, synthesized_path, sentences_per_label):\n",
    "        # todo: num_classes\n",
    "        h = Bert(num_classes = 2, random_state = seed)\n",
    "        train = pd.read_csv(train_path)\n",
    "        h.train(train.text, train.label,learning_rate=learning_rate,batch_size=batch_size,epochs=epochs) \n",
    "\n",
    "        with open(synthesized_path, \"r\") as file:\n",
    "            sentences = file.readlines()\n",
    "        \n",
    "        \n",
    "        labels = []\n",
    "        cleaned_sentences = []\n",
    "        for i,sentence in enumerate(sentences):\n",
    "            #sentence_parts = sentence.split(maxsplit = 1)\n",
    "            if len(sentence[3:]) <10:\n",
    "                continue\n",
    "            labels.append(sentence[:3])\n",
    "            cleaned_sentences.append(sentence[3:])\n",
    "\n",
    "\n",
    "        \n",
    "        predictions = h.predict_label_proba(pd.DataFrame(cleaned_sentences, columns=[\"text\"]))\n",
    "        pred = [pred[0] for pred in predictions]\n",
    "        conf = [pred[1] for pred in predictions]\n",
    "        data = {\"text\":cleaned_sentences,\"label\":labels,\"predicted label\":pred, \"confidence\":conf}\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        candidates = df.loc[df[\"label\"] == df[\"predicted label\"]]\n",
    "        candidates = candidates.sort_values([\"confidence\"],ascending=False).groupby([\"label\"]).head(sentences_per_label)    \n",
    "\n",
    "        return candidates[[\"text\", \"label\", \"confidence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 07:54:48.882367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10800 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7\n",
      "2021-11-11 07:54:52.448310: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 21s 260ms/step - loss: 0.6874 - accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.7369 - accuracy: 0.7000\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.5518 - accuracy: 0.6000\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.2172 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.1872 - accuracy: 1.0000\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "2\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 22s 291ms/step - loss: 0.6853 - accuracy: 0.7000\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 1s 279ms/step - loss: 0.7629 - accuracy: 0.5500\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 1s 277ms/step - loss: 0.4207 - accuracy: 0.8500\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 1s 279ms/step - loss: 0.2407 - accuracy: 0.9500\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 0.1949 - accuracy: 0.9500\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "6\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - 24s 279ms/step - loss: 0.9113 - accuracy: 0.5400\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 4s 273ms/step - loss: 1.0443 - accuracy: 0.4400\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 0.6221 - accuracy: 0.6200\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 0.4343 - accuracy: 0.8400\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 0.2037 - accuracy: 1.0000\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "12\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 27s 281ms/step - loss: 0.8631 - accuracy: 0.5200\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.6664 - accuracy: 0.5900\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 7s 279ms/step - loss: 0.4740 - accuracy: 0.7300\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.2916 - accuracy: 0.8900\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.0203 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for datapoints in [5,10,25,50]:\n",
    "    get_sentences(\"data/train_labeled_\"+str(datapoints) + \".csv\",\n",
    "                \"data/generated_samples_\" + str(datapoints)+\".txt\",\n",
    "                  datapoints).to_csv(\"data/filtered_data_\" + str(datapoints)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoints = 50\n",
    "train = pd.read_csv(\"data/filtered_data_\" + str(datapoints)+\".csv\")[[\"text\",\"label\"]]\n",
    "train = pd.concat([train,pd.read_csv(\"data/train_labeled_\" + str(datapoints)+\".csv\")]) \n",
    "train = train.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "\n",
    "label_map = {}\n",
    "for i,label in enumerate(train.label.unique()):\n",
    "    label_map[i] = label\n",
    "\n",
    "for number,label in label_map.items():\n",
    "    train.label[train.label==label] = number\n",
    "\n",
    "train.label = train.label.astype(\"int32\")\n",
    "train.label.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "50/50 [==============================] - 34s 281ms/step - loss: 0.6779 - accuracy: 0.6600\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 14s 280ms/step - loss: 0.3302 - accuracy: 0.9000\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 14s 280ms/step - loss: 0.1065 - accuracy: 0.9700\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 14s 280ms/step - loss: 2.3935e-04 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 14s 281ms/step - loss: 7.1746e-05 - accuracy: 1.0000\n",
      "500/500 [==============================] - 18s 33ms/step - loss: 1.5062 - accuracy: 0.7740\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_per_class</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_per_class  accuracy\n",
       "0         50.0     0.774"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(columns=[\"n_per_class\", \"accuracy\"])\n",
    "\n",
    "for datapoints in [5,10,25,50]:\n",
    "    h = Bert(num_classes = 2, random_state = seed)\n",
    "    train = pd.read_csv(\"data/filtered_data_\" + str(datapoints)+\".csv\")[[\"text\",\"label\"]]\n",
    "    train = pd.concat([pd.read_csv(\"data/train_labeled_\" + str(datapoints)+\".csv\"),train]) \n",
    "    train = train.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    train.label[0] = \"neg\"\n",
    "    h.train(train.text, train.label, learning_rate=learning_rate,batch_size=batch_size,epochs=epochs)  \n",
    "    performance = h.evaluate_from_path(\"data/test.csv\")[1]\n",
    "    row = {\"n_per_class\" : datapoints, \"accuracy\": performance}\n",
    "    data = data.append(row, ignore_index=True)\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results'):\n",
    "      os.mkdir('results')\n",
    "data.to_csv(\"results/LAMBADA_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
