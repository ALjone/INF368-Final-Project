{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q numpy\n",
    "!pip install -q pandas\n",
    "!pip install -q argparse\n",
    "!pip install -q torch\n",
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -q -U tensorflow-text\n",
    "!pip install -q tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed classes\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.chdir('..')\n",
    "from Bert import Bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import correct dataset and set hyperparmeters for GPT2\n",
    "\n",
    "Set data_name to the name of your dataset. This needs to correspond to a folder in /data/, which should be generated by the generate_data.ipynb notebook. num_classes manually needs to be set to the number of classes in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"imdb\"\n",
    "num_classes = 2\n",
    "\n",
    "#Other hyperparmaters and choices\n",
    "epochs = 5\n",
    "batch_size = 2\n",
    "device = \"cuda\"\n",
    "repeat_num = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune GPT2\n",
    "\n",
    "This cell will run the GPT2Tuner.py script with different arguments in order to fine tune GPT2 on the data, and then create the sentences. The sentences will then be saved to the correct folder to later filter out the good ones with BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Starting training:\n",
      "avg_train_loss 43.82683486938477\n",
      "elapsed time for 1 training epoch :  0:00:03\n",
      "avg_train_loss 4.536903810501099\n",
      "elapsed time for 1 training epoch :  0:00:03\n",
      "avg_train_loss 3.741558790206909\n",
      "elapsed time for 1 training epoch :  0:00:03\n",
      "avg_train_loss 3.2865145206451416\n",
      "elapsed time for 1 training epoch :  0:00:03\n",
      "avg_train_loss 2.968233323097229\n",
      "elapsed time for 1 training epoch :  0:00:03\n",
      "Generating sequences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sequences.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Starting training:\n",
      "avg_train_loss 14.667651486396789\n",
      "elapsed time for 1 training epoch :  0:00:08\n",
      "avg_train_loss 3.4145506024360657\n",
      "elapsed time for 1 training epoch :  0:00:07\n",
      "avg_train_loss 3.006114900112152\n",
      "elapsed time for 1 training epoch :  0:00:07\n",
      "avg_train_loss 2.751752698421478\n",
      "elapsed time for 1 training epoch :  0:00:07\n",
      "avg_train_loss 2.490455222129822\n",
      "elapsed time for 1 training epoch :  0:00:07\n",
      "Generating sequences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sequences.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1121 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Starting training:\n",
      "avg_train_loss 10.958024146556854\n",
      "elapsed time for 1 training epoch :  0:00:32\n",
      "avg_train_loss 2.1806747126579284\n",
      "elapsed time for 1 training epoch :  0:00:32\n",
      "avg_train_loss 2.0816142058372495\n",
      "elapsed time for 1 training epoch :  0:00:32\n",
      "avg_train_loss 1.9982030057907105\n",
      "elapsed time for 1 training epoch :  0:00:32\n",
      "avg_train_loss 1.8548086833953858\n",
      "elapsed time for 1 training epoch :  0:00:32\n",
      "Generating sequences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sequences.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1121 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Starting training:\n",
      "avg_train_loss 6.791853618621826\n",
      "elapsed time for 1 training epoch :  0:01:04\n",
      "avg_train_loss 2.024179359674454\n",
      "elapsed time for 1 training epoch :  0:01:04\n",
      "avg_train_loss 1.8546543788909913\n",
      "elapsed time for 1 training epoch :  0:01:04\n",
      "avg_train_loss 1.6954318547248841\n",
      "elapsed time for 1 training epoch :  0:01:04\n",
      "avg_train_loss 1.5748067557811738\n",
      "elapsed time for 1 training epoch :  0:01:04\n",
      "Generating sequences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sequences.\n"
     ]
    }
   ],
   "source": [
    "def generate_run_string(data_name,sample_number,epochs,batch_size,device,repeat_num,samples_per_class):\n",
    "    return f'python GPT2Tuner.py \\\n",
    "    --train_data_path data/{data_name}/train_labeled_{sample_number}.csv \\\n",
    "    --output_name generated_samples_{sample_number}.txt \\\n",
    "    --output_dir data/{data_name} \\\n",
    "    --epochs {epochs} \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --device {device} \\\n",
    "    --torch_seed 1 \\\n",
    "    --numpy_seed 2 \\\n",
    "    --random_seed 3 \\\n",
    "    --repeat_num {repeat_num} \\\n",
    "    --samples_per_class {samples_per_class}'\n",
    "\n",
    "for datapoints in [5, 10, 25, 50]:\n",
    "    !{generate_run_string(data_name = data_name, sample_number = datapoints, epochs = epochs, batch_size = batch_size, device = device, repeat_num = repeat_num, samples_per_class = datapoints)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters for BERT\n",
    "\n",
    "These are the hyperparameters used for the BERT classifier that will filter out the sentences we choose from the synthesized ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seed = 0\n",
    "learning_rate = 5e-5\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_sentences(train_path, synthesized_path, sentences_per_label):\n",
    "        \"\"\"This method trains a BERT classifier to choose the X best labels from among the data at synthesized_path\n",
    "        and returns them as a pandas dataframe, where X is sentences_per_label\"\"\"\n",
    "        #Train the BERT classifier that will be used to choose the sentences later\n",
    "        h = Bert(num_classes = num_classes, random_state = seed)\n",
    "        train = pd.read_csv(train_path)\n",
    "        h.train(train.text, train.label,learning_rate=learning_rate,batch_size=batch_size,epochs=epochs) \n",
    "        \n",
    "        #Read the data synthesized by GPT2\n",
    "        with open(synthesized_path, \"r\") as file:\n",
    "            sentences = file.readlines()\n",
    "        \n",
    "        #Split the data into labels and sentences\n",
    "        labels = []\n",
    "        cleaned_sentences = []\n",
    "        for i,sentence in enumerate(sentences):\n",
    "            #We split on the first space, as we know everything before it is the label\n",
    "            sentence_parts = sentence.split(maxsplit = 1)\n",
    "            if len(sentence_parts[1]) <10:\n",
    "                continue\n",
    "            labels.append(str(sentence_parts[0]))\n",
    "            cleaned_sentences.append(sentence_parts[1])\n",
    "\n",
    "\n",
    "        #Get confidence and prediction from BERT, and put it all into a pandas dataframe\n",
    "        predictions = h.predict_label_proba(pd.DataFrame(cleaned_sentences, columns=[\"text\"]))\n",
    "        pred = [str(pred[0]) for pred in predictions]\n",
    "        conf = [pred[1] for pred in predictions]\n",
    "        data = {\"text\":cleaned_sentences,\"label\":labels,\"predicted label\":pred, \"confidence\":conf}\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        #Do some pandas magic to only get the X best labels from each class (judged by the confidence of BERT)\n",
    "        #where X is sentences_per_label\n",
    "        candidates = df.loc[df[\"label\"] == df[\"predicted label\"]]\n",
    "        candidates = candidates.sort_values([\"confidence\"],ascending=False).groupby([\"label\"]).head(sentences_per_label)    \n",
    "        \n",
    "        #Return the dataframe but drop the \"predicted label\" as it will always be equal to label\n",
    "        return candidates[[\"text\", \"label\", \"confidence\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the best synthesized data\n",
    "\n",
    "Using the function above, we filter out the bad data and save the good data to a .csv that we can later to use train our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 19:10:02.227696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10800 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7\n",
      "2021-11-11 19:10:05.464888: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3/3 [==============================] - 21s 264ms/step - loss: 0.9356 - accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.7679 - accuracy: 0.6000\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.8391 - accuracy: 0.6000\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 1s 251ms/step - loss: 0.4762 - accuracy: 0.8000\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.5422 - accuracy: 0.8000\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 21s 290ms/step - loss: 0.7197 - accuracy: 0.6500\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 0.8451 - accuracy: 0.4500\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 0.4017 - accuracy: 0.9000\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 0.2644 - accuracy: 0.9500\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 0.2280 - accuracy: 1.0000\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - 24s 279ms/step - loss: 0.8160 - accuracy: 0.6200\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 1.0236 - accuracy: 0.4800\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 4s 272ms/step - loss: 0.8446 - accuracy: 0.4400\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 0.6907 - accuracy: 0.5600\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 0.5888 - accuracy: 0.6600\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 27s 283ms/step - loss: 0.8759 - accuracy: 0.5400\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 7s 281ms/step - loss: 0.6410 - accuracy: 0.6400\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 7s 281ms/step - loss: 0.3032 - accuracy: 0.8500\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.4469 - accuracy: 0.8400\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.1977 - accuracy: 0.9300\n"
     ]
    }
   ],
   "source": [
    "for datapoints in [5,10,25,50]:\n",
    "    get_sentences(f\"data/{data_name}/train_labeled_\"+str(datapoints) + \".csv\",\n",
    "                f\"data/{data_name}/generated_samples_\" + str(datapoints)+\".txt\",\n",
    "                  datapoints).to_csv(f\"data/{data_name}/filtered_data_\" + str(datapoints)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the final classifier on D_train and D_synthesized\n",
    "\n",
    "Here we train the final classifier on all the data together and asses its performance on the test set. We save the results to a .csv file for later inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 24s 293ms/step - loss: 1.3702 - accuracy: 0.3750\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 1s 280ms/step - loss: 0.4734 - accuracy: 0.8125\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 1s 280ms/step - loss: 0.4263 - accuracy: 0.8750\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 1s 277ms/step - loss: 0.2042 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 1s 279ms/step - loss: 0.2212 - accuracy: 0.9375\n",
      "500/500 [==============================] - 18s 33ms/step - loss: 0.7875 - accuracy: 0.4960\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "8/8 [==============================] - 26s 286ms/step - loss: 0.9356 - accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 2s 280ms/step - loss: 0.3971 - accuracy: 0.7812\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 2s 280ms/step - loss: 0.2551 - accuracy: 0.9062\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 2s 280ms/step - loss: 0.1283 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 2s 281ms/step - loss: 0.0454 - accuracy: 1.0000\n",
      "500/500 [==============================] - 18s 33ms/step - loss: 0.7128 - accuracy: 0.6480\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 31s 282ms/step - loss: 0.6332 - accuracy: 0.6600\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.3787 - accuracy: 0.8400\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.1807 - accuracy: 0.9500\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.1283 - accuracy: 0.9700\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.2191 - accuracy: 0.9400\n",
      "500/500 [==============================] - 18s 33ms/step - loss: 1.4963 - accuracy: 0.6740\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "50/50 [==============================] - 38s 281ms/step - loss: 0.7690 - accuracy: 0.5550\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 14s 280ms/step - loss: 0.4044 - accuracy: 0.8150\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 14s 280ms/step - loss: 0.0826 - accuracy: 0.9750\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 14s 281ms/step - loss: 0.0171 - accuracy: 0.9900\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 14s 280ms/step - loss: 2.5375e-04 - accuracy: 1.0000\n",
      "500/500 [==============================] - 18s 33ms/step - loss: 1.3160 - accuracy: 0.7840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_per_class</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_per_class  accuracy\n",
       "0          5.0     0.496\n",
       "1         10.0     0.648\n",
       "2         25.0     0.674\n",
       "3         50.0     0.784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(columns=[\"n_per_class\", \"accuracy\"])\n",
    "\n",
    "for datapoints in [5,10,25,50]:\n",
    "    #Train the classifier\n",
    "    h = Bert(num_classes = num_classes, random_state = seed)\n",
    "    train = pd.read_csv(f\"data/{data_name}/filtered_data_\" + str(datapoints)+\".csv\")\n",
    "    train = pd.concat([pd.read_csv(f\"data/{data_name}/train_labeled_\" + str(datapoints)+\".csv\"),train]) \n",
    "    h.train(train.text, train.label, learning_rate=learning_rate,batch_size=batch_size,epochs=epochs)  \n",
    "    \n",
    "    #Evaludate and save\n",
    "    performance = h.evaluate_from_path(f\"data/{data_name}/test.csv\")[1]\n",
    "    row = {\"n_per_class\" : datapoints, \"accuracy\": performance}\n",
    "    data = data.append(row, ignore_index=True)\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results\n",
    "\n",
    "Here we save the results to the correct folder so that everything is in one place and well organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists('results'):\n",
    "      os.mkdir('results')\n",
    "result_path = f'results/{data_name}'\n",
    "if not os.path.exists(result_path):\n",
    "      os.mkdir(result_path)\n",
    "data.to_csv(f\"{result_path}/LAMBADA_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
