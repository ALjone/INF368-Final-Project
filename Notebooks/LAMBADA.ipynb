{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.3.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install argparse\n",
    "!pip install torch\n",
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -q -U tensorflow-text\n",
    "!pip install -q tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed classes\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('..')\n",
    "from Bert import Bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import correct dataset and set hyperparmeters for GPT2\n",
    "\n",
    "Set data_name to the name of your dataset. This needs to correspond to a folder in /data/, which should be generated by the generate_data.ipynb notebook. num_classes manually needs to be set to the number of classes in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"imdb\"\n",
    "num_classes = 2\n",
    "\n",
    "#Other hyperparmaters and choices\n",
    "epochs = 5\n",
    "batch_size = 2\n",
    "device = \"cuda\"\n",
    "repeat_num = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune GPT2\n",
    "\n",
    "This cell will run the GPT2Tuner.py script with different arguments in order to fine tune GPT2 on the data, and then create the sentences. The sentences will then be saved to the correct folder to later filter out the good ones with BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Starting training:\n",
      "avg_train_loss 43.82683486938477\n",
      "elapsed time for 1 training epoch :  0:00:03\n",
      "avg_train_loss 4.536903810501099\n",
      "elapsed time for 1 training epoch :  0:00:03\n",
      "avg_train_loss 3.741558790206909\n",
      "elapsed time for 1 training epoch :  0:00:03\n",
      "avg_train_loss 3.2865145206451416\n",
      "elapsed time for 1 training epoch :  0:00:03\n",
      "avg_train_loss 2.968233323097229\n",
      "elapsed time for 1 training epoch :  0:00:03\n",
      "Generating sequences\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Finished generating sequences.\n"
     ]
    }
   ],
   "source": [
    "def generate_run_string(data_name,sample_number,epochs,batch_size,device,repeat_num,samples_per_class):\n",
    "    return f'python GPT2Tuner.py \\\n",
    "    --train_data_path data/{data_name}/train_labeled_{sample_number}.csv \\\n",
    "    --output_name generated_samples_{sample_number}.txt \\\n",
    "    --output_dir data/{data_name} \\\n",
    "    --epochs {epochs} \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --device {device} \\\n",
    "    --torch_seed 1 \\\n",
    "    --numpy_seed 2 \\\n",
    "    --random_seed 3 \\\n",
    "    --repeat_num {repeat_num} \\\n",
    "    --samples_per_class {samples_per_class}'\n",
    "\n",
    "for datapoints in [5, 10, 25, 50]:\n",
    "    !{generate_run_string(data_name = data_name, sample_number = datapoints, epochs = epochs, batch_size = batch_size,\n",
    "                          device = device, repeat_num = repeat_num, samples_per_class = datapoints)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters for BERT\n",
    "\n",
    "These are the hyperparameters used for the BERT classifier that will filter out the sentences we choose from the synthesized ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seed = 0\n",
    "learning_rate = 5e-5\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_sentences(train_path, synthesized_path, sentences_per_label):\n",
    "        \"\"\"This method trains a BERT classifier to choose the X best labels from among the data at synthesized_path\n",
    "        and returns them as a pandas dataframe, where X is sentences_per_label\"\"\"\n",
    "        #Train the BERT classifier that will be used to choose the sentences later\n",
    "        h = Bert(num_classes = num_classes, random_state = seed)\n",
    "        train = pd.read_csv(train_path)\n",
    "        h.train(train.text, train.label,learning_rate=learning_rate,batch_size=batch_size,epochs=epochs) \n",
    "        \n",
    "        #Read the data synthesized by GPT2\n",
    "        with open(synthesized_path, \"r\") as file:\n",
    "            sentences = file.readlines()\n",
    "        \n",
    "        #Split the data into labels and sentences\n",
    "        labels = []\n",
    "        cleaned_sentences = []\n",
    "        for i,sentence in enumerate(sentences):\n",
    "            #We split on the first space, as we know everything before it is the label\n",
    "            sentence_parts = sentence.split(maxsplit = 1)\n",
    "            if len(sentence_parts[1]) <10:\n",
    "                continue\n",
    "            labels.append(sentence_parts[0])\n",
    "            cleaned_sentences.append(sentence_parts[1])\n",
    "\n",
    "\n",
    "        #Get confidence and prediction from BERT, and put it all into a pandas dataframe\n",
    "        predictions = h.predict_label_proba(pd.DataFrame(cleaned_sentences, columns=[\"text\"]))\n",
    "        pred = [pred[0] for pred in predictions]\n",
    "        conf = [pred[1] for pred in predictions]\n",
    "        data = {\"text\":cleaned_sentences,\"label\":labels,\"predicted label\":pred, \"confidence\":conf}\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        #Do some pandas magic to only get the X best labels from each class (judged by the confidence of BERT)\n",
    "        #where X is sentences_per_label\n",
    "        candidates = df.loc[df[\"label\"] == df[\"predicted label\"]]\n",
    "        candidates = candidates.sort_values([\"confidence\"],ascending=False).groupby([\"label\"]).head(sentences_per_label)    \n",
    "        \n",
    "        #Return the dataframe but drop the \"predicted label\" as it will always be equal to label\n",
    "        return candidates[[\"text\", \"label\", \"confidence\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the best synthesized data\n",
    "\n",
    "Using the function above, we filter out the bad data and save the good data to a .csv that we can later to use train our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 07:54:48.882367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10800 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7\n",
      "2021-11-11 07:54:52.448310: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 21s 260ms/step - loss: 0.6874 - accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.7369 - accuracy: 0.7000\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.5518 - accuracy: 0.6000\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.2172 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.1872 - accuracy: 1.0000\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "2\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 22s 291ms/step - loss: 0.6853 - accuracy: 0.7000\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 1s 279ms/step - loss: 0.7629 - accuracy: 0.5500\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 1s 277ms/step - loss: 0.4207 - accuracy: 0.8500\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 1s 279ms/step - loss: 0.2407 - accuracy: 0.9500\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 0.1949 - accuracy: 0.9500\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "6\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - 24s 279ms/step - loss: 0.9113 - accuracy: 0.5400\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 4s 273ms/step - loss: 1.0443 - accuracy: 0.4400\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 0.6221 - accuracy: 0.6200\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 0.4343 - accuracy: 0.8400\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 0.2037 - accuracy: 1.0000\n",
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "12\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 27s 281ms/step - loss: 0.8631 - accuracy: 0.5200\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.6664 - accuracy: 0.5900\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 7s 279ms/step - loss: 0.4740 - accuracy: 0.7300\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.2916 - accuracy: 0.8900\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 0.0203 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for datapoints in [5,10,25,50]:\n",
    "    get_sentences(\"data/train_labeled_\"+str(datapoints) + \".csv\",\n",
    "                \"data/generated_samples_\" + str(datapoints)+\".txt\",\n",
    "                  datapoints).to_csv(\"data/filtered_data_\" + str(datapoints)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the final classifier on D_train and D_synthesized\n",
    "\n",
    "Here we train the final classifier on all the data together and asses its performance on the test set. We save the results to a .csv file for later inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n",
      "Epoch 1/5\n",
      "50/50 [==============================] - 34s 281ms/step - loss: 0.6779 - accuracy: 0.6600\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 14s 280ms/step - loss: 0.3302 - accuracy: 0.9000\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 14s 280ms/step - loss: 0.1065 - accuracy: 0.9700\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 14s 280ms/step - loss: 2.3935e-04 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 14s 281ms/step - loss: 7.1746e-05 - accuracy: 1.0000\n",
      "500/500 [==============================] - 18s 33ms/step - loss: 1.5062 - accuracy: 0.7740\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_per_class</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_per_class  accuracy\n",
       "0         50.0     0.774"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(columns=[\"n_per_class\", \"accuracy\"])\n",
    "\n",
    "for datapoints in [5,10,25,50]:\n",
    "    #Train the classifier\n",
    "    h = Bert(num_classes = num_classes, random_state = seed)\n",
    "    train = pd.read_csv(\"data/filtered_data_\" + str(datapoints)+\".csv\")[[\"text\",\"label\"]]\n",
    "    train = pd.concat([pd.read_csv(\"data/train_labeled_\" + str(datapoints)+\".csv\"),train]) \n",
    "    h.train(train.text, train.label, learning_rate=learning_rate,batch_size=batch_size,epochs=epochs)  \n",
    "    \n",
    "    #Evaludate and save\n",
    "    performance = h.evaluate_from_path(\"data/test.csv\")[1]\n",
    "    row = {\"n_per_class\" : datapoints, \"accuracy\": performance}\n",
    "    data = data.append(row, ignore_index=True)\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results\n",
    "\n",
    "Here we save the results to the correct folder so that everything is in one place and well organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists('results'):\n",
    "      os.mkdir('results')\n",
    "result_path = f'results/{data_name}'\n",
    "if not os.path.exists(result_path):\n",
    "      os.mkdir(result_path)\n",
    "data.to_csv(f\"{result_path}/LAMBADA_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
