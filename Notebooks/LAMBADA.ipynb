{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!git clone https://github.com/ALjone/INF368-Final-Project\n",
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -q -U tensorflow-text\n",
    "!pip install -q tf-models-official\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(\"/content/INF368-Final-Project\")\n",
    "from Bert import Bert\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_5.csv \\\n",
    "--output_name generated_samples_5.txt \\\n",
    "--epochs 10 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg_train_loss 10.755517369508743\n",
      "elapsed time for 1 training epoch :  0:01:43\n",
      "avg_train_loss 2.440124177932739\n",
      "elapsed time for 1 training epoch :  0:01:35\n",
      "avg_train_loss 1.9912845328450204\n",
      "elapsed time for 1 training epoch :  0:01:35\n",
      "avg_train_loss 1.8234325051307678\n",
      "elapsed time for 1 training epoch :  0:01:36\n",
      "avg_train_loss 1.6345902353525161\n",
      "elapsed time for 1 training epoch :  0:01:35\n",
      "Generating sentences\n",
      "Finished generating sentences.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_10.csv \\\n",
    "--output_name generated_samples_10.txt \\\n",
    "--epochs 10 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"GPT2Tuner.py\", line 142, in <module>\n",
      "    tuner.save_sentences(int(args.samples_per_class), path=args.output_dir + \"/\" + args.output_name)\n",
      "  File \"GPT2Tuner.py\", line 113, in save_sentences\n",
      "    f.write(self.tokenizer.decode(sample_output, skip_special_tokens=True).replace(\"\\n\", \"\")+\"\\n\")\n",
      "  File \"D:\\Anaconda\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\x96' in position 396: character maps to <undefined>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg_train_loss 5.252679090499878\n",
      "elapsed time for 1 training epoch :  0:10:09\n",
      "avg_train_loss 1.7242548483610154\n",
      "elapsed time for 1 training epoch :  0:10:03\n",
      "avg_train_loss 1.6241241866350173\n",
      "elapsed time for 1 training epoch :  0:09:44\n",
      "avg_train_loss 1.470165878534317\n",
      "elapsed time for 1 training epoch :  0:10:09\n",
      "avg_train_loss 1.2941617393493652\n",
      "elapsed time for 1 training epoch :  0:09:03\n",
      "avg_train_loss 1.1303855109214782\n",
      "elapsed time for 1 training epoch :  0:09:02\n",
      "avg_train_loss 0.9625671485066414\n",
      "elapsed time for 1 training epoch :  0:09:03\n",
      "avg_train_loss 0.8146653261780739\n",
      "elapsed time for 1 training epoch :  0:09:11\n",
      "avg_train_loss 0.6817608812451362\n",
      "elapsed time for 1 training epoch :  0:11:03\n",
      "avg_train_loss 0.5570713968575001\n",
      "elapsed time for 1 training epoch :  0:16:43\n",
      "Generating sentences\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_25.csv \\\n",
    "--output_name generated_samples_25.txt \\\n",
    "--epochs 10 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:\n",
      "avg_train_loss 3.7483344689011573\n",
      "elapsed time for 1 training epoch :  0:12:29\n",
      "avg_train_loss 1.6732652413845062\n",
      "elapsed time for 1 training epoch :  0:14:36\n",
      "avg_train_loss 1.2771680292487144\n",
      "elapsed time for 1 training epoch :  0:19:47\n",
      "avg_train_loss 1.023910652846098\n",
      "elapsed time for 1 training epoch :  0:27:10\n",
      "avg_train_loss 0.8440950420498848\n",
      "elapsed time for 1 training epoch :  0:49:47\n",
      "Generating sentences\n",
      "Finished generating sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "!python GPT2Tuner.py \\\n",
    "--train_data_path data/train_labeled_50.csv \\\n",
    "--output_name generated_samples_50.txt \\\n",
    "--epochs 10 \\\n",
    "--batch_size 2 \\\n",
    "--device cuda \\\n",
    "--torch_seed 1 \\\n",
    "--numpy_seed 2 \\\n",
    "--random_seed 3 \\\n",
    "--repeat_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seed = 0\n",
    "learning_rate = 5e-5\n",
    "epochs=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_sentences(train_path, synthesized_path, sentences_per_label):\n",
    "        h = Bert(num_classes = 2, random_state = seed)\n",
    "        train = pd.read_csv(train_path)\n",
    "        h.train(train.text, train.label,learning_rate=learning_rate,batch_size=batch_size,epochs=epochs) \n",
    "\n",
    "        with open(synthesized_path, \"r\") as file:\n",
    "            sentences = file.readlines()\n",
    "        \n",
    "        #TODO get the X best of each label, not just top X best total\n",
    "\n",
    "        #TODO also gather data in dict before putting it in dataframe\n",
    "        #https://stackoverflow.com/questions/57000903/what-is-the-fastest-and-most-efficient-way-to-append-rows-to-a-dataframe\n",
    "\n",
    "        labels = []\n",
    "        cleaned_sentences = []\n",
    "        for sentence in sentences:\n",
    "            #sentence_parts = sentence.split(maxsplit = 1)\n",
    "            labels.append(sentence[:3])\n",
    "            cleaned_sentences.append(sentence[3:])\n",
    "\n",
    "        df = pd.DataFrame(columns=[\"text\", \"label\", \"predicted label\", \"confidence\"])\n",
    "        for label, sentence in zip(labels, cleaned_sentences):\n",
    "            predictions = h.predict_label_proba([sentence])[0]\n",
    "            new_row = {\"text\": sentence, \"label\": label, \"predicted label\" : predictions[0], \"confidence\" : predictions[1]}\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "        \n",
    "        candidates = df.loc[df[\"label\"] == df[\"predicted label\"]]\n",
    "        candidates.sort_values(\"confidence\", ascending=False, inplace=True)\n",
    "        #Make sure we have enough labels\n",
    "        if candidates.size >= sentences_per_label*len(set(labels)):\n",
    "            return candidates.head(sentences_per_label*len(set(labels)))[[\"text\", \"label\", \"confidence\"]]\n",
    "        else:\n",
    "            print(\"Not enough correct labels synthesized, returning what little we have\")\n",
    "            return candidates[[\"text\", \"label\", \"confidence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datapoints in [5, 10]:\n",
    "  get_sentences(\"/content/INF368-Final-Project/data/train_labeled_\"+str(datapoints) + \".csv\",\n",
    "                \"/content/INF368-Final-Project/data/generated_samples_\" + str(datapoints)+\".txt\", 10).to_csv(\"/content/INF368-Final-Project/data/filtered_data_\" + str(datapoints)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "data = pd.DataFrame(columns=[\"n_per_class\", \"accuracy\"])\n",
    "for datapoints in [5, 10]:\n",
    "  h = Bert(num_classes = 2, random_state = seed)\n",
    "  train = pd.read_csv(\"/content/INF368-Final-Project/data/filtered_data_\" + str(datapoints)+\".csv\")\n",
    "  h.train(train.text, train.label,learning_rate=learning_rate,batch_size=batch_size,epochs=epochs)  \n",
    "  performance = h.evaluate_from_path(\"/content/INF368-Final-Project/data/test.csv\")[1]\n",
    "  row = {\"n_per_class\" : datapoints, \"accuracy\": performance}\n",
    "  data = data.append(row, ignore_index=True)\n",
    "data\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "      os.mkdir('results')\n",
    "data.to_csv(\"results/LAMBADA_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}